{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL-LunaLnder-finalProj.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HND9HYhOE89T"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-cVn8qQKtJq"
      },
      "source": [
        "Based on the following links to create this notebook:\n",
        "\n",
        "https://colab.research.google.com/drive/18LdlDDT87eb8cCTHZsXyS9ksQPzL3i6H\n",
        "\n",
        "https://colab.research.google.com/drive/1tug_bpg8RwrFOI8C6Ed-zo0OgD3yfnWy#scrollTo=bhsj7BTPHepg\n",
        "\n",
        "https://colab.research.google.com/drive/1tug_bpg8RwrFOI8C6Ed-zo0OgD3yfnWy\n",
        "\n",
        "\n",
        "To run Gym, you have to install prerequisites like xvbf,opengl & other python-dev packages using the following codes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14NwgxVmX-Og"
      },
      "source": [
        "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmtbAjPPKiFw",
        "outputId": "9685ae8c-2083-4835-ca84-62801d5096e5"
      },
      "source": [
        "!pip install gym  torch\n",
        "!apt-get install python-opengl ffmpeg -y \n",
        "!apt install xvfb -y \n",
        "!pip install pyvirtualdisplay  \n",
        "!pip install piglet \n",
        "!pip install gym[box2d] \n",
        "!pip install tensorflow \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.7.1+cu101)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2LD1gwzX-Oh"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_hxdz4ALVQ3"
      },
      "source": [
        "# This code creates a virtual display to draw game images on. \n",
        "# If you are running locally, just ignore it\n",
        "import os\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
        "    !bash ../xvfb start\n",
        "    %env DISPLAY=:1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1GqN0iRLaZk"
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) # error only\n",
        "# import tensorflow as tf\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "from collections import namedtuple"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILbriB_oLn5T"
      },
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env\n",
        "\n",
        "def make_noise():\n",
        "    mu, sigma = 0, 0.05 # mean and standard deviation\n",
        "    return np.random.normal(mu, sigma, 1)[0]\n",
        "\n",
        "def plot(frame_idx, rewards, losses):\n",
        "#     clear_output(True)\n",
        "    plt.figure(figsize=(20,5))\n",
        "    plt.subplot(131)\n",
        "    plt.title('frame %s. reward: %s' % (frame_idx, np.mean(rewards[-10:])))\n",
        "    plt.plot(rewards)\n",
        "    plt.subplot(132)\n",
        "    plt.title('loss')\n",
        "    plt.plot(losses)\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SERZbDJqX-Oj"
      },
      "source": [
        "# My Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LU2kLfDwX-Ok"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyVG3_erX-Ok"
      },
      "source": [
        "class defaultActions:\n",
        "    def __init__(self):\n",
        "\n",
        "        self.main = [[0 , \"Main off\"],\n",
        "                     [0.5 ,\"Main slow\"],\n",
        "#                      [0.75 , \"Main meduim\"],\n",
        "                     [1    , \"Main strong\"]\n",
        "                    ]\n",
        "        self.left_right = [\n",
        "                        [0, \"Left Right off\"],\n",
        "#                         [-0.5 , \"Left slow\"],\n",
        "#                         [-0.75  , \"Left meduim\"],\n",
        "                        [-1 , \"Left strong\"],\n",
        "                        \n",
        "#                         [0.5 , \"Right slow\"],\n",
        "#                         [0.75  , \"Right meduim\"],\n",
        "                        [1 , \"Right strong\"],\n",
        "                        ]\n",
        "        self.all_actions = {}\n",
        "        i = 0  \n",
        "        for main_eng in self.main:\n",
        "            for sec_eng in self.left_right:\n",
        "#                 print (f\"a{i},act:[{main_eng[0]},{sec_eng[0]},{main_eng[1]} {sec_eng[1]}]\")\n",
        "                self.all_actions[i] = [[main_eng[0],sec_eng[0]] , f\"{main_eng[1]}, {sec_eng[1]}\"]\n",
        "                i+=1\n",
        "        \n",
        "    def get_full_action(self,id):\n",
        "        return self.all_actions[id]\n",
        "\n",
        "    def get_action(self,id,add_noise=True):\n",
        "        if add_noise :\n",
        "            return [self.all_actions[id][0][0]+make_noise(), self.all_actions[id][0][1]+make_noise()]\n",
        "        else:\n",
        "            return self.all_actions[id][0]\n",
        "\n",
        "    def get_description(self,id):\n",
        "        return self.all_actions[id][1]\n",
        "    def get_action_count(self):\n",
        "        return len(self.all_actions)\n",
        "actions = defaultActions()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbb1iMa0X-Ol"
      },
      "source": [
        "class model(nn.Module):\n",
        "    def __init__(self, layers, name=\"\"):\n",
        "        super(model, self).__init__()\n",
        "        self.name = \"\"\n",
        "        self.layers = layers\n",
        "        self.features = nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "    def name(self):\n",
        "        return self.name\n",
        "\n",
        "    def model_summery(self):\n",
        "        return self.features.summary()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.features(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ld8W5xvqX-Ol"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DafsbPV3X-Ol"
      },
      "source": [
        "input_size = 8\n",
        "h1 = 128\n",
        "h2 = 128\n",
        "h3 = 128 \n",
        "h4 = 128\n",
        "h5 = 128\n",
        "output_size = actions.get_action_count()\n",
        "layers = [nn.Linear(input_size,h1) ,nn.ReLU(inplace=True), #, nn.BatchNorm1d(h1)\n",
        "         nn.Linear(h1,h2) ,nn.ReLU(inplace=True),\n",
        "#          nn.Linear(h2,h3) ,nn.ReLU(inplace=True),\n",
        "#          nn.Linear(h3,h4) , nn.ReLU(inplace=True),\n",
        "#          nn.Linear(h4,h5) ,nn.ReLU(inplace=True),\n",
        "         nn.Linear(h5,output_size),nn.ReLU(inplace=True) # , nn.BatchNorm1d(output_size)\n",
        "         ]\n",
        "for x in layers:\n",
        "    if isinstance(x, nn.Linear):\n",
        "        nn.init.normal_(x.weight, mean=0, std=1.0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp4NQ6inX-Ol"
      },
      "source": [
        "\n",
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCHLYkMkLzcf"
      },
      "source": [
        "# Box2d Environment\n",
        "env = gym.make('LunarLanderContinuous-v2')\n",
        "env.reset()\n",
        "plt.imshow(env.render('rgb_array'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wh4v_3zCRepL"
      },
      "source": [
        "state_size = env.observation_space\n",
        "print(\"state size is:\", state_size)\n",
        "a = env.action_space\n",
        "print(\"action size=\",a) \n",
        "state = env.reset()\n",
        "print(state)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTfAdl7dSS-K"
      },
      "source": [
        "# Action Space\n",
        "            #is two floats [main engine, left-right engines].\n",
        "            # Main engine: -1..0 off, 0..+1 throttle from 50% to 100% power. Engine can't work with less than 50% power.\n",
        "            # Left-right:  -1.0..-0.5 fire left engine, +0.5..+1.0 fire right engine, -0.5..0.5 off\n",
        "            self.action_space = spaces.Box(-1, +1, (2,), dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-g_F9GGEX-On"
      },
      "source": [
        "DF = 0.9 #discount_factor\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 200\n",
        "max_iterations = 3000\n",
        "steps_done = 0\n",
        "TARGET_UPDATE = 10\n",
        "BATCH_SIZE = 40\n",
        "GAMMA = 0.999\n",
        "# torch.autograd.set_detect_anomaly(True)\n",
        "#mytestmodel.zero_grad()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeuwA3RqX-On"
      },
      "source": [
        "policy_net =  model(layers).to(device)\n",
        "target_net =  model(layers).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOLFojNKX-On"
      },
      "source": [
        "optimizer = optim.Adadelta(policy_net.parameters(), lr=0.3 )#5e-6 RMSprop\n",
        "dtype = torch.float\n",
        "losses = []\n",
        "all_rewards = []\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQaFvJdvM4qX"
      },
      "source": [
        "losses = []\n",
        "all_rewards = []\n",
        "for a in range(100):\n",
        "    memory = ReplayMemory(10000)\n",
        "    print(\"Episode: \",a,\"\\n\",\"-\"*50)\n",
        "    env = gym.make('LunarLanderContinuous-v2')\n",
        "    env.reset()\n",
        "    env = wrap_env(env)\n",
        "    done = False\n",
        "    iter = 0\n",
        "#     print(done)\n",
        "    observation = state = env.reset()\n",
        "\n",
        "    action = 1\n",
        "    TotalReward = 0\n",
        "    TotalLoss=0\n",
        "    df = DF\n",
        "\n",
        "    while not done and iter < max_iterations :\n",
        "      iter +=1\n",
        "      \n",
        "    #   action = env.action_space.sample()\n",
        "      if len(memory) < BATCH_SIZE:\n",
        "\n",
        "          sample = random.random()\n",
        "          eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
        "          steps_done += 1\n",
        "          state_values = policy_net.forward(torch.from_numpy(observation))\n",
        "          if sample > eps_threshold:\n",
        "                # t.max(1) will return largest column value of each row.\n",
        "                # second column on max result is index of where max element was\n",
        "                # found, so we pick action with the larger expected reward.\n",
        "              real_action = state_values.argmax()\n",
        "\n",
        "          else:\n",
        "#               print(\"random sample\")\n",
        "              real_action = torch.tensor(random.randrange(actions.get_action_count()), device=device, dtype=torch.long)\n",
        "\n",
        "\n",
        "          action = actions.get_action(real_action.item())\n",
        "          observation, reward, done, _ = env.step(action)\n",
        "          with torch.no_grad():\n",
        "              next_values = target_net.forward(torch.from_numpy(observation))\n",
        "          if done and True:\n",
        "              new_reward = next_values.max()+0.1\n",
        "          else:\n",
        "            observation, new_reward, done, _ = env.step(action)\n",
        "    #       state_values[torch.isnan(state_values)] = 0\n",
        "    #       next_values[torch.isnan(next_values)] = 0\n",
        "          next_state = observation\n",
        "          next_val = next_values.max() if next_values.max()!=0 and next_values.max() is not None else -0.1\n",
        "\n",
        "          label = torch.tensor(reward + (df* new_reward), dtype=torch.float)\n",
        "          df *=df\n",
        "\n",
        "          print(iter,\"action is:\",actions.get_full_action(real_action.item()) ,\n",
        "                 \"reward: \",reward,\"full:\",label,\n",
        "                \"state_value:\",state_values,\n",
        "                \"max opt\",state_values.gather(-1,real_action))\n",
        "          memory.push(state, action, next_state, reward)\n",
        "          continue\n",
        "#           print(real_action.unsqueeze(0))\n",
        "#           loss = F.smooth_l1_loss(state_values.gather(-1,real_action),label ) # Huber .unsqueeze(0)\n",
        "      else:\n",
        "          transitions = memory.sample(BATCH_SIZE)\n",
        "          batch = Transition(*zip(*transitions))\n",
        "          non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
        "          print(f\"iter {iter}\")\n",
        "          state_batch = batch.state\n",
        "          action_batch = batch.action\n",
        "          reward_batch = batch.reward\n",
        "          non_final_next_states = torch.tensor([s for s in batch.next_state if s is not None])\n",
        "          state_values =[]\n",
        "          with torch.no_grad():\n",
        "              for i in range(len(state_batch)):\n",
        "                  our_s = policy_net.forward(torch.from_numpy(state_batch[i])).tolist()\n",
        "                  state_values.append(our_s)\n",
        "          state_values = torch.as_tensor(state_values)\n",
        "          motor_controls = state_values.clone().detach().numpy()\n",
        "    #       state_values[torch.isnan(state_values)] = 0\n",
        "    #       next_values[torch.isnan(next_values)] = 0\n",
        "          next_val = next_values.max() if next_values.max()!=0 and next_values.max() is not None else -0.1\n",
        "          # state_values = torch.cat(state_values)\n",
        "          # observation, reward, done, _ = env.step(action_batch[0])\n",
        "          # state_values = policy_net.forward(torch.from_numpy(observation))\n",
        "          # print(\"action is:\",actions.get_full_action(state_values.argmax().item())[0] ,\"reward: \",reward)\n",
        "          # next_values = target_net.forward(torch.from_numpy(observation))\n",
        "          # real_action = actions.get_full_action(state_values.argmax().item())[0]\n",
        "          next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "          next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
        "          expected_state_action_values = (next_state_values * GAMMA) + torch.tensor(reward_batch)\n",
        "          print(f\"hedva { expected_state_action_values.unsqueeze(1)}\")\n",
        "          loss = F.smooth_l1_loss(state_values, expected_state_action_values.unsqueeze(1))\n",
        "          TotalLoss += loss.data.item()\n",
        "          TotalReward+= sum(reward_batch)\n",
        "          # Optimize the model\n",
        "          optimizer.zero_grad()\n",
        "          print(f\"loss {loss}\")\n",
        "          print(f\"loss.data {loss.data}\")\n",
        "          loss.requres_grad = True\n",
        "          loss.backward()\n",
        "          for param in policy_net.parameters():\n",
        "              param.grad.data.clamp_(-1, 1)\n",
        "          optimizer.step()\n",
        "          # if done and True:\n",
        "          #     new_reward = next_values.max()+0.1\n",
        "          # else:\n",
        "          #     observation, new_reward, done, _ = env.step(action)\n",
        "          # label = torch.tensor(reward + (df* new_reward), dtype=torch.float)\n",
        "      # loss = F.smooth_l1_loss(state_values[real_action],label ) # Huber .unsqueeze(0)\n",
        "\n",
        "#           print (loss.data)\n",
        "      # TotalLoss += loss.data.item()\n",
        "        \n",
        "      optimizer.zero_grad()\n",
        "#           if loss != None:\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "\n",
        "      # TotalReward+= reward\n",
        "      #print(\"state is:\", observation)\n",
        "\n",
        "      #if you want to see results on real-time 'open' the following 4 lines\n",
        "      if iter % TARGET_UPDATE/2 == 0:\n",
        "          screen = env.render(mode='rgb_array')\n",
        "          plt.imshow(screen)\n",
        "          ipythondisplay.clear_output(wait=True)\n",
        "          ipythondisplay.display(plt.gcf())\n",
        "\n",
        "\n",
        "      if iter % TARGET_UPDATE == 0:\n",
        "            target_net.load_state_dict(policy_net.state_dict())\n",
        "            \n",
        "    print(TotalReward,iter,done)\n",
        "    all_rewards.append(TotalReward)\n",
        "    losses.append(TotalLoss/iter)\n",
        "    print(\"-\"*50)\n",
        "    if  a % TARGET_UPDATE*5 == 0:\n",
        "        ipythondisplay.clear_output(wait=True)\n",
        "        pickle.dump(policy_net, open(f\"model.pkl\", \"wb\"))\n",
        "        plot(a, all_rewards, losses)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzyI3vKZQiR6"
      },
      "source": [
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY_ve2aViqJa"
      },
      "source": [
        "print(iter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQSL6GAC3Dw3"
      },
      "source": [
        "#Draw random samples from a normal (Gaussian) distribution.\n",
        "mu, sigma = 0, 0.05 # mean and standard deviation\n",
        "s = np.random.normal(mu, sigma, 1)\n",
        "print(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2N8kYsjX-Ow"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RngQ0ZUX-Ox"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScmN3t6LX-Ox"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}