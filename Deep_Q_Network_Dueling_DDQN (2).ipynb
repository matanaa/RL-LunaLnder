{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Deep_Q_Network_Dueling_DDQN.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMYtDRA6Kdwe"
      },
      "source": [
        "# Dueling Double Deep Q-Network (DDDQN)\n",
        "---\n",
        "Implementation of the agent with OpenAI Gym's LunarLander-v2 environment. The code is based on materials from Udacity Deep Reinforcement Learning Nanodegree Program. \n",
        "\n",
        "### 1. Import the Necessary Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHe_Fl2yNXvW"
      },
      "source": [
        "The Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gFlvs_RfhFE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c4afb74-2521-4fcf-aa36-a1946835d714"
      },
      "source": [
        "!pip install gym  torch\r\n",
        "!apt-get install python-opengl ffmpeg -y \r\n",
        "!apt install xvfb -y \r\n",
        "!pip install pyvirtualdisplay  \r\n",
        "!pip install piglet \r\n",
        "!pip install gym[box2d] \r\n",
        "!pip install tensorflow "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.0+cu101)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "Suggested packages:\n",
            "  libgle3\n",
            "The following NEW packages will be installed:\n",
            "  python-opengl\n",
            "0 upgraded, 1 newly installed, 0 to remove and 29 not upgraded.\n",
            "Need to get 496 kB of archives.\n",
            "After this operation, 5,416 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n",
            "Fetched 496 kB in 1s (676 kB/s)\n",
            "Selecting previously unselected package python-opengl.\n",
            "(Reading database ... 160975 files and directories currently installed.)\n",
            "Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-1) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 29 not upgraded.\n",
            "Need to get 784 kB of archives.\n",
            "After this operation, 2,270 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.8 [784 kB]\n",
            "Fetched 784 kB in 1s (1,119 kB/s)\n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 163330 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.8_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.8) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading https://files.pythonhosted.org/packages/19/88/7a198a5ee3baa3d547f5a49574cd8c3913b216f5276b690b028f89ffb325/PyVirtualDisplay-2.1-py3-none-any.whl\n",
            "Collecting EasyProcess\n",
            "  Downloading https://files.pythonhosted.org/packages/48/3c/75573613641c90c6d094059ac28adb748560d99bd27ee6f80cce398f404e/EasyProcess-0.3-py2.py3-none-any.whl\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
            "Successfully installed EasyProcess-0.3 pyvirtualdisplay-2.1\n",
            "Collecting piglet\n",
            "  Downloading https://files.pythonhosted.org/packages/11/56/6840e5f45626dc7eb7cd5dff57d11880b3113723b3b7b1fb1fa537855b75/piglet-1.0.0-py2.py3-none-any.whl\n",
            "Collecting piglet-templates\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/1e/49d7e0df9420eeb13a636487b8e606cf099f2ee0793159edd8ffe905125b/piglet_templates-1.1.0-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.1MB/s \n",
            "\u001b[?25hCollecting Parsley\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/d6/4fed8d65e28a970e1c5cb33ce9c7e22e3de745e1b2ae37af051ef16aea3b/Parsley-1.3-py2.py3-none-any.whl (88kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 4.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (1.6.3)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (20.3.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (1.1.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse->piglet-templates->piglet) (0.36.2)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from astunparse->piglet-templates->piglet) (1.15.0)\n",
            "Installing collected packages: Parsley, piglet-templates, piglet\n",
            "Successfully installed Parsley-1.3 piglet-1.0.0 piglet-templates-1.1.0\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]) (1.3.0)\n",
            "Collecting box2d-py~=2.3.5; extra == \"box2d\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/34/da5393985c3ff9a76351df6127c275dcb5749ae0abbe8d5210f06d97405d/box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]) (0.16.0)\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.4.1)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.10.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.4.1)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow) (54.0.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (0.4.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (3.3.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.27.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.7.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4t3hsZjfFEd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "399e2043-5ed1-4312-af4b-38a47e151f5b"
      },
      "source": [
        "from pyvirtualdisplay import Display\r\n",
        "display = Display(visible=0, size=(1400, 900))\r\n",
        "display.start()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f6f7aeb40d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OkdWAHAfHZH"
      },
      "source": [
        "# This code creates a virtual display to draw game images on. \r\n",
        "# If you are running locally, just ignore it\r\n",
        "import os\r\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\r\n",
        "    !bash ../xvfb start\r\n",
        "    %env DISPLAY=:1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DADFdcHwfJEV"
      },
      "source": [
        "import gym\r\n",
        "from gym import logger as gymlogger\r\n",
        "from gym.wrappers import Monitor\r\n",
        "gymlogger.set_level(40) # error only\r\n",
        "# import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "import random\r\n",
        "import matplotlib\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline\r\n",
        "import math\r\n",
        "import glob\r\n",
        "import io\r\n",
        "import base64\r\n",
        "from IPython.display import HTML\r\n",
        "\r\n",
        "from IPython import display as ipythondisplay\r\n",
        "from collections import namedtuple\r\n",
        "from itertools import count"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXcyP8h8fNdo"
      },
      "source": [
        "\"\"\"\r\n",
        "Utility functions to enable video recording of gym environment and displaying it\r\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\r\n",
        "\"\"\"\r\n",
        "\r\n",
        "def show_video():\r\n",
        "  mp4list = glob.glob('video/*.mp4')\r\n",
        "  if len(mp4list) > 0:\r\n",
        "    mp4 = mp4list[0]\r\n",
        "    video = io.open(mp4, 'r+b').read()\r\n",
        "    encoded = base64.b64encode(video)\r\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \r\n",
        "                loop controls style=\"height: 400px;\">\r\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\r\n",
        "             </video>'''.format(encoded.decode('ascii'))))\r\n",
        "  else: \r\n",
        "    print(\"Could not find video\")\r\n",
        "    \r\n",
        "\r\n",
        "def wrap_env(env):\r\n",
        "  env = Monitor(env, './video', force=True)\r\n",
        "  return env"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRGqigKVhvos"
      },
      "source": [
        "class defaultActions:\r\n",
        "    def __init__(self):\r\n",
        "\r\n",
        "        self.main = [[1    , \"Main strong\"],\r\n",
        "                     [0.51 ,\"Main slow\"],\r\n",
        "#                      [0.75 , \"Main meduim\"],\r\n",
        "                     [0 , \"Main off\"]\r\n",
        "                    ]\r\n",
        "        self.left_right = [\r\n",
        "                        [0, \"Left Right off\"],\r\n",
        "                        [-0.5 , \"Left slow\"],\r\n",
        "#                         [-0.75  , \"Left meduim\"],\r\n",
        "                        [-1 , \"Left strong\"],\r\n",
        "                        \r\n",
        "                        [0.5 , \"Right slow\"],\r\n",
        "#                         [0.75  , \"Right meduim\"],\r\n",
        "                        [1 , \"Right strong\"],\r\n",
        "                        ]\r\n",
        "        self.all_actions = {}\r\n",
        "        i = 0  \r\n",
        "        for main_eng in self.main:\r\n",
        "            for sec_eng in self.left_right:\r\n",
        "#                 print (f\"a{i},act:[{main_eng[0]},{sec_eng[0]},{main_eng[1]} {sec_eng[1]}]\")\r\n",
        "                self.all_actions[i] = [[main_eng[0],sec_eng[0]] , f\"{main_eng[1]}, {sec_eng[1]}\"]\r\n",
        "                i+=1\r\n",
        "        \r\n",
        "    def get_full_action(self,id):\r\n",
        "        return self.all_actions[id]\r\n",
        "\r\n",
        "    def get_action(self,id,add_noise=False):\r\n",
        "        if add_noise :\r\n",
        "            return [self.all_actions[id][0][0]+make_noise(), self.all_actions[id][0][1]+make_noise()]\r\n",
        "        else:\r\n",
        "            return self.all_actions[id][0]\r\n",
        "\r\n",
        "    def get_description(self,id):\r\n",
        "        return self.all_actions[id][1]\r\n",
        "    def get_action_count(self):\r\n",
        "        return len(self.all_actions)\r\n",
        "actions = defaultActions()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BojW9TcgLJ8v"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "class QNetwork(nn.Module):\r\n",
        "    \"\"\"Actor (Policy) Model.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, state_size, action_size, seed, fc1_size = 64, fc2_size = 64):\r\n",
        "        \"\"\"Initialize parameters and build model.\r\n",
        "        Params\r\n",
        "        ======\r\n",
        "            state_size (int): Dimension of each state\r\n",
        "            action_size (int): Dimension of each action\r\n",
        "            seed (int): Random seed\r\n",
        "        \"\"\"\r\n",
        "        \r\n",
        "        super(QNetwork, self).__init__()\r\n",
        "        hidden_size = 30\r\n",
        "        self.seed = torch.manual_seed(seed)\r\n",
        "        self.fc1 = nn.Linear(state_size, fc1_size)\r\n",
        "        self.fc2 = nn.Linear(fc1_size, fc2_size)\r\n",
        "        self.out = nn.Linear(fc2_size, action_size)\r\n",
        "        \r\n",
        "        \"*** YOUR CODE HERE ***\"\r\n",
        "\r\n",
        "    def forward(self, state):\r\n",
        "        \"\"\"Build a network that maps state -> action values.\"\"\"\r\n",
        "        x = self.fc1(state)\r\n",
        "        x = F.relu(x)\r\n",
        "        x = self.fc2(x)\r\n",
        "        x = F.relu(x)\r\n",
        "        action = self.out(x)\r\n",
        "        return action\r\n",
        "    \r\n",
        "class DuelingQNetwork(nn.Module):\r\n",
        "    \"\"\"Actor (Policy) Model.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, state_size, action_size, seed, fc1_size = 64, fc2_size = 64):\r\n",
        "        \"\"\"Initialize parameters and build model.\r\n",
        "        Params\r\n",
        "        ======\r\n",
        "            state_size (int): Dimension of each state\r\n",
        "            action_size (int): Dimension of each action\r\n",
        "            seed (int): Random seed\r\n",
        "        \"\"\"\r\n",
        "        \r\n",
        "        super(DuelingQNetwork, self).__init__()\r\n",
        "        self.num_actions = action_size\r\n",
        "        fc3_1_size = fc3_2_size = 32\r\n",
        "        self.seed = torch.manual_seed(seed)\r\n",
        "        self.fc1 = nn.Linear(state_size, fc1_size)\r\n",
        "        self.fc2 = nn.Linear(fc1_size, fc2_size)\r\n",
        "        ## Here we separate into two streams\r\n",
        "        # The one that calculate V(s)\r\n",
        "        self.fc3_1 = nn.Linear(fc2_size, fc3_1_size)\r\n",
        "        self.fc4_1 = nn.Linear(fc3_1_size, 1)\r\n",
        "        # The one that calculate A(s,a)\r\n",
        "        self.fc3_2 = nn.Linear(fc2_size, fc3_2_size)\r\n",
        "        self.fc4_2 = nn.Linear(fc3_2_size, action_size)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    def forward(self, state):\r\n",
        "        \"\"\"Build a network that maps state -> action values.\"\"\"\r\n",
        "        x = F.relu(self.fc1(state))\r\n",
        "        x = F.relu(self.fc2(x))\r\n",
        "\r\n",
        "        val = F.relu(self.fc3_1(x))\r\n",
        "        val = self.fc4_1(val)\r\n",
        "        \r\n",
        "        adv = F.relu(self.fc3_2(x))\r\n",
        "        adv = self.fc4_2(adv)\r\n",
        "        # Q(s,a) = V(s) + (A(s,a) - 1/|A| * sum A(s,a'))\r\n",
        "        action = val + adv - adv.mean(1).unsqueeze(1).expand(state.size(0), self.num_actions)\r\n",
        "        return action"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0CVwzczNcxG"
      },
      "source": [
        "The agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Tj5_nX3K9RX"
      },
      "source": [
        "import numpy as np\r\n",
        "import random\r\n",
        "from collections import namedtuple, deque\r\n",
        "\r\n",
        "# from model import QNetwork, DuelingQNetwork\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "\r\n",
        "BUFFER_SIZE = int(1e5)  # replay buffer size\r\n",
        "BATCH_SIZE = 16        # minibatch size\r\n",
        "GAMMA = 0.99            # discount factor\r\n",
        "TAU = 1e-3              # for soft update of target parameters\r\n",
        "LR = 0.000368               # learning rate \r\n",
        "UPDATE_EVERY = 4        # how often to update the network\r\n",
        "\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "\r\n",
        "class Agent():\r\n",
        "    \"\"\"Interacts with and learns from the environment.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, state_size, action_size, seed):\r\n",
        "        \"\"\"Initialize an Agent object.\r\n",
        "        \r\n",
        "        Params\r\n",
        "        ======\r\n",
        "            state_size (int): dimension of each state\r\n",
        "            action_size (int): dimension of each action\r\n",
        "            seed (int): random seed\r\n",
        "        \"\"\"\r\n",
        "        self.state_size = state_size\r\n",
        "        self.action_size = action_size\r\n",
        "        self.seed = random.seed(seed)\r\n",
        "\r\n",
        "        # Q-Network\r\n",
        "        self.qnetwork_local = DuelingQNetwork(state_size, action_size, seed).to(device)\r\n",
        "        self.qnetwork_target = DuelingQNetwork(state_size, action_size, seed).to(device)\r\n",
        "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\r\n",
        "\r\n",
        "        # Replay memory\r\n",
        "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\r\n",
        "        # Initialize time step (for updating every UPDATE_EVERY steps)\r\n",
        "        self.t_step = 0\r\n",
        "    \r\n",
        "    def step(self, state, action, reward, next_state, done):\r\n",
        "        # Save experience in replay memory\r\n",
        "        self.memory.add(state, action, reward, next_state, done)\r\n",
        "        \r\n",
        "        # Learn every UPDATE_EVERY time steps.\r\n",
        "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\r\n",
        "        if self.t_step == 0:\r\n",
        "            # If enough samples are available in memory, get random subset and learn\r\n",
        "            if len(self.memory) > BATCH_SIZE:\r\n",
        "                experiences = self.memory.sample()\r\n",
        "                self.learn_DDQN(experiences, GAMMA)\r\n",
        "\r\n",
        "    def act(self, state, eps=0.):\r\n",
        "        \"\"\"Returns actions for given state as per current policy.\r\n",
        "        \r\n",
        "        Params\r\n",
        "        ======\r\n",
        "            state (array_like): current state\r\n",
        "            eps (float): epsilon, for epsilon-greedy action selection\r\n",
        "        \"\"\"\r\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\r\n",
        "        self.qnetwork_local.eval()\r\n",
        "        with torch.no_grad():\r\n",
        "            action_values = self.qnetwork_local(state)\r\n",
        "        self.qnetwork_local.train()\r\n",
        "\r\n",
        "        # Epsilon-greedy action selection\r\n",
        "        if random.random() > eps:\r\n",
        "            return np.argmax(action_values.cpu().data.numpy())\r\n",
        "        else:\r\n",
        "            return random.choice(np.arange(self.action_size))\r\n",
        "\r\n",
        "    def learn(self, experiences, gamma):\r\n",
        "        \"\"\"Update value parameters using given batch of experience tuples.\r\n",
        "        Params\r\n",
        "        ======\r\n",
        "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \r\n",
        "            gamma (float): discount factor\r\n",
        "        \"\"\"\r\n",
        "        states, actions, rewards, next_states, dones = experiences\r\n",
        "        # Get max predicted Q values (for next states) from target model\r\n",
        "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\r\n",
        "        # Compute Q targets for current states \r\n",
        "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\r\n",
        "\r\n",
        "        # Get expected Q values from local model\r\n",
        "        Q_expected = self.qnetwork_local(states).gather(1, actions)\r\n",
        "\r\n",
        "        # Compute loss\r\n",
        "        loss = F.mse_loss(Q_expected, Q_targets)\r\n",
        "        # Minimize the loss\r\n",
        "        self.optimizer.zero_grad()\r\n",
        "        loss.backward()\r\n",
        "        self.optimizer.step()\r\n",
        "        \r\n",
        "        # ------------------- update target network ------------------- #\r\n",
        "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)  \r\n",
        "        \r\n",
        "       \r\n",
        "    def learn_DDQN(self, experiences, gamma):\r\n",
        "        \"\"\"Update value parameters using given batch of experience tuples.\r\n",
        "        Params\r\n",
        "        ======\r\n",
        "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \r\n",
        "            gamma (float): discount factor\r\n",
        "        \"\"\"\r\n",
        "        states, actions, rewards, next_states, dones = experiences\r\n",
        "        # Get index of maximum value for next state from Q_expected\r\n",
        "        Q_argmax = self.qnetwork_local(next_states).detach()\r\n",
        "        _, a_prime = Q_argmax.max(1)\r\n",
        "        #print (self.qnetwork_local(states).detach())\r\n",
        "        # Get max predicted Q values (for next states) from target model\r\n",
        "        Q_targets_next = self.qnetwork_target(next_states).detach().gather(1, a_prime.unsqueeze(1))\r\n",
        "        #print (Q_targets_next.shape)\r\n",
        "        # Compute Q targets for current states \r\n",
        "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\r\n",
        "        #print (Q_targets.shape)\r\n",
        "        # Get expected Q values from local model\r\n",
        "        Q_expected = self.qnetwork_local(states).gather(1, actions)\r\n",
        "        #print (Q_expected.shape)\r\n",
        "        # Compute loss\r\n",
        "        loss = F.mse_loss(Q_expected, Q_targets)\r\n",
        "        # Minimize the loss\r\n",
        "        self.optimizer.zero_grad()\r\n",
        "        loss.backward()\r\n",
        "        self.optimizer.step()\r\n",
        "\r\n",
        "        # ------------------- update target network ------------------- #\r\n",
        "        self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)  \r\n",
        "\r\n",
        "    def soft_update(self, local_model, target_model, tau):\r\n",
        "        \"\"\"Soft update model parameters.\r\n",
        "        θ_target = τ*θ_local + (1 - τ)*θ_target\r\n",
        "        Params\r\n",
        "        ======\r\n",
        "            local_model (PyTorch model): weights will be copied from\r\n",
        "            target_model (PyTorch model): weights will be copied to\r\n",
        "            tau (float): interpolation parameter \r\n",
        "        \"\"\"\r\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\r\n",
        "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\r\n",
        "\r\n",
        "\r\n",
        "class ReplayBuffer:\r\n",
        "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\r\n",
        "        \"\"\"Initialize a ReplayBuffer object.\r\n",
        "        Params\r\n",
        "        ======\r\n",
        "            action_size (int): dimension of each action\r\n",
        "            buffer_size (int): maximum size of buffer\r\n",
        "            batch_size (int): size of each training batch\r\n",
        "            seed (int): random seed\r\n",
        "        \"\"\"\r\n",
        "        self.action_size = action_size\r\n",
        "        self.memory = deque(maxlen=buffer_size)  \r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\r\n",
        "        self.seed = random.seed(seed)\r\n",
        "    \r\n",
        "    def add(self, state, action, reward, next_state, done):\r\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\r\n",
        "        e = self.experience(state, action, reward, next_state, done)\r\n",
        "        self.memory.append(e)\r\n",
        "    \r\n",
        "    def sample(self):\r\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\r\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\r\n",
        "\r\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\r\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\r\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\r\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\r\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\r\n",
        "  \r\n",
        "        return (states, actions, rewards, next_states, dones)\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\r\n",
        "        return len(self.memory)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eacwk4DHKdwk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2876f34d-d141-49e6-ea45-a1fb8632aea8"
      },
      "source": [
        "import gym\n",
        "!pip install box2d\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "!python -m pip install pyvirtualdisplay\n",
        "#from pyvirtualdisplay import Display\n",
        "#display = Display(visible=0, size=(1400, 900))\n",
        "#display.start()\n",
        "\n",
        "is_ipython = 'inline' in plt.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting box2d\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/1b/ce95bb5d1807d4d85af8d0c90050add1a77124459f8097791f0c39136d53/Box2D-2.3.10-cp37-cp37m-manylinux1_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 5.6MB/s \n",
            "\u001b[?25hInstalling collected packages: box2d\n",
            "Successfully installed box2d-2.3.10\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.7/dist-packages (2.1)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.7/dist-packages (from pyvirtualdisplay) (0.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FPxhO-GKdwl"
      },
      "source": [
        "### 2. Instantiate the Environment and Agent\n",
        "\n",
        "Initialize the environment in the code cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uuc1b9-Kdwm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba24348a-9411-4588-a4d8-b8753493fdcc"
      },
      "source": [
        "# env = gym.make('LunarLander-v2')\n",
        "# env.seed(0)\n",
        "env = gym.make('LunarLanderContinuous-v2')\n",
        "env.seed(0)\n",
        "env.reset()\n",
        "env = wrap_env(env)\n",
        "np.random.seed(0)\n",
        "print('State shape: ', env.observation_space.shape)\n",
        "# print('Number of actions: ', env.action_space.n)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "State shape:  (8,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rS8E5JyhKdwm"
      },
      "source": [
        "### 3. Train the Agent \n",
        "\n",
        "Run the code cell below to train the agent from scratch. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouFojCRcKdwn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "442b59a3-d94c-4447-fec9-da416ac13361"
      },
      "source": [
        "# from dqn_agent import Agent\n",
        "\n",
        "agent = Agent(state_size=8, action_size=actions.get_action_count(), seed=0)\n",
        "\n",
        "def train(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
        "    \"\"\"Deep Q-Learning.\n",
        "    \n",
        "    Params\n",
        "    ======\n",
        "        n_episodes (int): maximum number of training episodes\n",
        "        max_t (int): maximum number of timesteps per episode\n",
        "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
        "        eps_end (float): minimum value of epsilon\n",
        "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
        "    \"\"\"\n",
        "    scores = []                        # list containing scores from each episode\n",
        "    scores_window = deque(maxlen=100)  # last 100 scores\n",
        "    eps = eps_start                    # initialize epsilon\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "        for t in range(max_t):\n",
        "            action = agent.act(state, eps)\n",
        "            action_step = actions.get_action(action)\n",
        "            next_state, reward, done, _ = env.step(action_step)\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            if done:\n",
        "                break \n",
        "        scores_window.append(score)       # save most recent score\n",
        "        scores.append(score)              # save most recent score\n",
        "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
        "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
        "        if i_episode % 100 == 0:\n",
        "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "        if np.mean(scores_window)>=200.0:\n",
        "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
        "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint_Dueling_DDQN.pth')\n",
        "            break\n",
        "    return scores\n",
        "\n",
        "scores = train()\n",
        "\n",
        "# plot the scores\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "plt.plot(np.arange(len(scores)), scores)\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Episode #')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 17\tAverage Score: -344.77"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lm8vhT2JKdwn"
      },
      "source": [
        "### 4. Watch a Smart Agent!\n",
        "\n",
        "In the next code cell, you will load the trained weights from file to watch a smart agent!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKYbncoeKdwo"
      },
      "source": [
        "# from dqn_agent import Agent\n",
        "\n",
        "agent = Agent(state_size=8, action_size=actions.get_action_count(), seed=0)\n",
        "# load the weights from file\n",
        "agent.qnetwork_local.load_state_dict(torch.load('checkpoint_Dueling_DDQN.pth', map_location=lambda storage, loc: storage))\n",
        "sum = 0\n",
        "for i in range(200):\n",
        "    print(f\"episode {i}\")\n",
        "    state = env.reset()\n",
        "    Total_reward = 0\n",
        "    if i >= 199:\n",
        "        img = plt.imshow(env.render(mode='rgb_array'))\n",
        "    for j in range(3000):\n",
        "        action = agent.act(state)\n",
        "        if i >= 199:\n",
        "            img.set_data(env.render(mode='rgb_array')) \n",
        "            # plt.axis('off')\n",
        "            display.display(plt.gcf())\n",
        "            display.clear_output(wait=True)\n",
        "        action_step = actions.get_action(action)        \n",
        "        state, reward, done, _ = env.step(action_step)\n",
        "        Total_reward += reward\n",
        "        if done:\n",
        "            break \n",
        "    sum += Total_reward\n",
        "avg = sum / 200\n",
        "print(f\"avg: {avg}\")\n",
        "    \n",
        "show_video()            \n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaYs_FE5Kdwo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}